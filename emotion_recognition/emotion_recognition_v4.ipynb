{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09153f79",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35dcf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras import regularizers\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c2859",
   "metadata": {},
   "source": [
    "### View FER-2013 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f9a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       angry  happy  neutral\n",
      "train   3995   7215     4965\n",
      "      angry  happy  neutral\n",
      "test    958   1774     1233\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"./emotions/train/\"\n",
    "test_dir = \"./emotions/test/\"\n",
    "\n",
    "row, col = 48, 48\n",
    "classes = 3\n",
    "\n",
    "def count_data(path, set_):\n",
    "    dict_ = {}\n",
    "    for emotion in os.listdir(path):\n",
    "        if not emotion.startswith('.') and emotion != 'sad':\n",
    "            dir_ = path + emotion\n",
    "            dict_[emotion] = len(os.listdir(dir_))\n",
    "    df = pd.DataFrame(dict_, index = [set_])\n",
    "    return df\n",
    "\n",
    "train_count = count_data(train_dir, 'train')\n",
    "test_count = count_data(test_dir, 'test')\n",
    "print(train_count)\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b66c2",
   "metadata": {},
   "source": [
    "### Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c5cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16175 images belonging to 3 classes.\n",
      "Found 3965 images belonging to 3 classes.\n",
      "{'angry': 0, 'happy': 1, 'neutral': 2}\n",
      "{'angry': 0, 'happy': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(rescale = 1. / 255, zoom_range = 0.3, horizontal_flip = True)\n",
    "train_set = train_gen.flow_from_directory(train_dir,\n",
    "                                          classes = ['angry', 'happy', 'neutral'],\n",
    "                                          batch_size = 64, \n",
    "                                          target_size = (row, col),\n",
    "                                          shuffle = True,\n",
    "                                          color_mode = 'grayscale', \n",
    "                                          class_mode = 'categorical')\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale = 1. / 255)\n",
    "test_set = test_gen.flow_from_directory(test_dir, \n",
    "                                        classes = ['angry', 'happy', 'neutral'],\n",
    "                                        batch_size = 64,\n",
    "                                        target_size = (row, col), \n",
    "                                        shuffle = True, \n",
    "                                        color_mode = 'grayscale', \n",
    "                                        class_mode = 'categorical')\n",
    "\n",
    "print(train_set.class_indices)\n",
    "print(test_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3beab5",
   "metadata": {},
   "source": [
    "### Create the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "975c19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_54 (Conv2D)          (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 48, 48, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_55 (Conv2D)          (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_55 (Bat  (None, 48, 48, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 24, 24, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_56 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_56 (Bat  (None, 24, 24, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_57 (Bat  (None, 24, 24, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 12, 12, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_58 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 12, 12, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 12, 12, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1024)              4719616   \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,739\n",
      "Trainable params: 5,663,843\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'emotions_v4.hd5'\n",
    "\n",
    "def build_model(model_name, input_size, classes = 3):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)\n",
    "    else:      \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = input_size))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = 2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(64, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = 2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = 2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(1024, activation = 'relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(512, activation = 'relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(256, activation = 'relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(classes, activation = 'softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(MODEL_NAME, (row, col, 1), classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d2c15",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78604c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Epoch 1/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.7913\n",
      "Epoch 1: val_loss improved from inf to 0.48446, saving model to emotions_v4.hd5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: emotions_v4.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: emotions_v4.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 92s 361ms/step - loss: 0.5080 - accuracy: 0.7913 - val_loss: 0.4845 - val_accuracy: 0.8040 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.5043 - accuracy: 0.7937\n",
      "Epoch 2: val_loss did not improve from 0.48446\n",
      "252/252 [==============================] - 90s 356ms/step - loss: 0.5043 - accuracy: 0.7937 - val_loss: 0.4948 - val_accuracy: 0.8064 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4921 - accuracy: 0.7950\n",
      "Epoch 3: val_loss improved from 0.48446 to 0.47704, saving model to emotions_v4.hd5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: emotions_v4.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: emotions_v4.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 90s 356ms/step - loss: 0.4921 - accuracy: 0.7950 - val_loss: 0.4770 - val_accuracy: 0.8071 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4914 - accuracy: 0.8005\n",
      "Epoch 4: val_loss did not improve from 0.47704\n",
      "252/252 [==============================] - 88s 350ms/step - loss: 0.4914 - accuracy: 0.8005 - val_loss: 0.4925 - val_accuracy: 0.8117 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4824 - accuracy: 0.8029\n",
      "Epoch 5: val_loss did not improve from 0.47704\n",
      "252/252 [==============================] - 91s 361ms/step - loss: 0.4824 - accuracy: 0.8029 - val_loss: 0.5079 - val_accuracy: 0.8028 - lr: 1.0000e-04\n",
      "Epoch 6/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.8047\n",
      "Epoch 6: val_loss improved from 0.47704 to 0.47571, saving model to emotions_v4.hd5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: emotions_v4.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: emotions_v4.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 89s 352ms/step - loss: 0.4785 - accuracy: 0.8047 - val_loss: 0.4757 - val_accuracy: 0.8117 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4766 - accuracy: 0.8057\n",
      "Epoch 7: val_loss did not improve from 0.47571\n",
      "252/252 [==============================] - 92s 365ms/step - loss: 0.4766 - accuracy: 0.8057 - val_loss: 0.5067 - val_accuracy: 0.8066 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4763 - accuracy: 0.8056\n",
      "Epoch 8: val_loss did not improve from 0.47571\n",
      "252/252 [==============================] - 90s 357ms/step - loss: 0.4763 - accuracy: 0.8056 - val_loss: 0.4875 - val_accuracy: 0.8066 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "252/252 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.8098\n",
      "Epoch 9: val_loss did not improve from 0.47571\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "252/252 [==============================] - 91s 361ms/step - loss: 0.4667 - accuracy: 0.8098 - val_loss: 0.4874 - val_accuracy: 0.8140 - lr: 1.0000e-04\n",
      "Epoch 9: early stopping\n",
      "Done. Now evaluating.\n",
      "62/62 [==============================] - 5s 75ms/step - loss: 0.4761 - accuracy: 0.8119\n",
      "Test accuracy: 0.81, loss: 0.48\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_set, test_set, epochs, model_name):\n",
    "    model.compile(optimizer = Adam(learning_rate = 0.0001, decay = 1e-6), \n",
    "                  loss = 'categorical_crossentropy', \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    savemodel = ModelCheckpoint(filepath = model_name,\n",
    "                                save_best_only = True,\n",
    "                                verbose = 1,\n",
    "                                mode = 'min',\n",
    "                                monitor = 'val_loss')\n",
    "    stopmodel = EarlyStopping(monitor = 'val_loss',\n",
    "                              min_delta = 0.001, \n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              restore_best_weights = True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                                  factor = 0.2, \n",
    "                                  patience = 6, \n",
    "                                  verbose = 1, \n",
    "                                  min_delta = 0.001)\n",
    "    \n",
    "    steps_per_epoch = train_set.n // train_set.batch_size\n",
    "    validation_steps = test_set.n // test_set.batch_size\n",
    "    \n",
    "    print(\"Starting training.\")\n",
    "    \n",
    "    model.fit(x = train_set, \n",
    "              validation_data = test_set, \n",
    "              epochs = epochs, \n",
    "              callbacks = [savemodel, stopmodel, reduce_lr],\n",
    "              steps_per_epoch = steps_per_epoch, \n",
    "              validation_steps = validation_steps)\n",
    "    \n",
    "    print(\"Done. Now evaluating.\")\n",
    "    loss, acc = model.evaluate(test_set)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\" % (acc, loss))\n",
    "    \n",
    "train(model, train_set, test_set, 10, MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
